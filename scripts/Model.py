from math import ceil
import os
import tensorflow as tf
import numpy as np
from tensorflow._api.v2.config import optimizer

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)
tf.config.run_functions_eagerly(True)


class CNNBlock(tf.keras.layers.Layer):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1):
        super(CNNBlock, self).__init__()

        self.conv = tf.keras.layers.Conv2D(
            out_channels, kernel_size, stride, padding, groups=groups, use_bias=False)
        self.bn = tf.keras.layers.BatchNormalization(out_channels)
        self.silu = tf.keras.layers.Activation(tf.keras.activations.swish)

    def call(self, inputs, training=False):
        return self.silu(self.bn(self.conv(inputs), training=training))


class SqueezeExcitationBlock(tf.keras.layers.Layer):
    def __init__(self, in_channels, reduced_dim):
        super(SqueezeExcitationBlock, self).__init__()
        self.sequential = tf.keras.Sequential([
            tf.keras.layers.GlobalAveragePooling2D(),
            tf.keras.layers.Conv2D(
                reduced_dim, kernel_size=3, activation=tf.keras.activations.swish),
            tf.keras.layers.Conv2D(
                in_channels, kernel_size=3, activation=tf.keras.activations.sigmoid)
        ])

    def call(self, inputs):
        return inputs * self.sequential(inputs)


class InvertedResidualBlock(tf.keras.layers.Layer):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, expand_ratio):
        super(InvertedResidualBlock, self).__init__()
        self.stride = stride
        self.use_residual = self.stride == 1 and in_channels == out_channels

        hidden_dim = in_channels * expand_ratio
        self.expand = in_channels != hidden_dim
        if self.expand:
            self.expand_conv = CNNBlock(
                in_channels, hidden_dim, kernel_size=1, stride=1, padding="valid")

        self.conv = tf.keras.Sequential([
            CNNBlock(hidden_dim, hidden_dim, kernel_size,
                     stride, padding, groups=hidden_dim),
            SqueezeExcitationBlock(hidden_dim, hidden_dim // 4),
            tf.keras.layers.Conv2D(
                out_channels, kernel_size=1, padding="valid", use_bias=False),
            tf.keras.layers.BatchNormalization(out_channels)
        ])

    def call(self, inputs, training=False):
        x = self.expand_conv(
            inputs, training=training) if self.expand else inputs
        if self.use_residual:
            return self.conv(x, training=training) + inputs
        else:
            return self.conv(x, training=training)


class EfficientNet(tf.keras.Model):
    def __init__(self, phi_values, num_classes):
        super(EfficientNet, self).__init__()
        # expand_ratio, channels, repeats, stride, kernel_size
        self.base_model = [
            [1, 16, 1, 1, 3],
            [6, 24, 2, 2, 3],
            [6, 40, 2, 2, 5],
            [6, 80, 3, 2, 3],
            [6, 112, 3, 1, 5],
            [6, 192, 4, 2, 5],
            [6, 320, 1, 1, 3],
        ]
        width_factor, depth_factor, dropout_rate = self.calculate_factors(
            phi_values)
        last_channels = ceil(1280 * width_factor)
        self.pool = tf.keras.layers.GlobalAveragePooling2D()
        self.features = self.create_features(
            width_factor, depth_factor, last_channels)
        self.classifer = tf.keras.Sequential([
            tf.keras.layers.Dropout(dropout_rate),
            tf.keras.layers.Dense(int(
                last_channels/4), activation=tf.keras.activations.swish),
            tf.keras.layers.Dense(
                num_classes, activation=tf.keras.activations.softmax),
        ])

    def calculate_factors(self, phi_values, alpha=1.2, beta=1.1):
        phi, res, drop_rate = phi_values[0], phi_values[1], phi_values[2]
        depth_factor = alpha ** phi
        width_factor = beta ** phi
        return width_factor, depth_factor, drop_rate

    def create_features(self, width_factor, depth_factor, last_channels):
        channels = int(32*width_factor)
        features = [CNNBlock(3, channels, 3, stride=2, padding="same")]
        in_channels = channels

        for expand_ratio, channels, repeats, stride, kernel_size in self.base_model:
            out_channels = 4*ceil(int(channels*width_factor) / 4)
            layers_repeats = ceil(repeats * depth_factor)

            for layer in range(layers_repeats):
                features.append(
                    InvertedResidualBlock(
                        in_channels,
                        out_channels,
                        expand_ratio=expand_ratio,
                        stride=stride if layer == 0 else 1,
                        kernel_size=kernel_size,
                        padding="same",  # if k=1:pad=0, k=3:pad=1, k=5:pad=2
                    )
                )
                in_channels = out_channels

        features.append(CNNBlock(in_channels, last_channels,
                        kernel_size=1, stride=1, padding="valid"))
        return tf.keras.Sequential(features)

    def call(self, x):
        x = self.pool(self.features(x))
        return self.classifier(x.view(x.shape[0], -1))


if __name__ == "__main__":
    model = EfficientNet([6, 224, 0.5], 9)
    model.build(input_shape=(128, 3, 224, 224))
    model.compile(
        optimizer=tf.keras.optimizers.Adam(),
        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
        metrics=["accuracy"],
    )
    model.summary()
